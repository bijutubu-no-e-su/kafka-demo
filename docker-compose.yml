services:
  # --- このファイルは docker compose で各サービスをまとめて起動する定義です ---
  # サービス名は compose ネットワーク内の DNS 名（例: kafka, pg, connect, ksqldb, ui）になります。
  # 各サービスは内部ホスト名で相互に通信する想定です（例: connect -> pg, connect -> kafka）。
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    platform: linux/amd64
    container_name: kafka
    ports:
      - '29092:29092' # ホスト→localhost:29092
    environment:
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:9092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:29092'
      # KAFKA_LISTENERS: 'PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:29093,PLAINTEXT_HOST://0.0.0.0:29092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092'
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    volumes:
      - kafkadata:/var/lib/kafka/data
    command: >
      bash -lc '
        set -e

        # 1) CLUSTER_ID を用意（未設定なら生成）
        if [ -z "$${CLUSTER_ID:-}" ]; then
          if command -v kafka-storage >/dev/null 2>&1; then
            export CLUSTER_ID=$$(kafka-storage random-uuid)
          else
            export CLUSTER_ID=$$(kafka-storage.sh random-uuid)
          fi
          echo "[kafka] generated CLUSTER_ID=$$CLUSTER_ID"
        else
          echo "[kafka] using provided CLUSTER_ID=$$CLUSTER_ID"
        fi

        # 2) env→/etc/kafka/kafka.properties 生成
        /etc/confluent/docker/configure

        # 3) 初回のみフォーマット
        if [ ! -f "/var/lib/kafka/data/meta.properties" ]; then
          echo "[kafka] formatting storage with CLUSTER_ID=$$CLUSTER_ID..."
          if command -v kafka-storage >/dev/null 2>&1; then
            kafka-storage format --ignore-formatted -t "$$CLUSTER_ID" -c /etc/kafka/kafka.properties
          else
            kafka-storage.sh format --ignore-formatted -t "$$CLUSTER_ID" -c /etc/kafka/kafka.properties
          fi
        fi

        # 4) 通常起動
        exec /etc/confluent/docker/run
      '
    healthcheck:
      test:
        [
          'CMD',
          'bash',
          '-lc',
          'kafka-topics --bootstrap-server kafka:9092 --list >/dev/null 2>&1 || exit 1',
        ]
      interval: 10s
      timeout: 5s
      retries: 30
  pg:
    # Debezium が推奨する Postgres 連携イメージ。logical replication に必要な設定が含まれる
    image: debezium/postgres:16-alpine
    container_name: pg
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=demo
    ports:
      - '5432:5432'
    volumes:
      # 起動時に init スクリプトを実行してスキーマ/サンプルデータを投入
      - ./init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U postgres -d demo']
      interval: 5s
      timeout: 5s
      retries: 30
  connect:
    image: debezium/connect:2.6
    platform: linux/amd64
    container_name: connect
    depends_on:
      kafka:
        condition: service_healthy
      pg:
        condition: service_healthy
    ports:
      - '8083:8083'
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: connect-cluster
      CONFIG_STORAGE_TOPIC: '_connect_configs'
      OFFSET_STORAGE_TOPIC: '_connect_offsets'
      STATUS_STORAGE_TOPIC: '_connect_status'
      KEY_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      KEY_CONVERTER_SCHEMAS_ENABLE: 'false'
      VALUE_CONVERTER_SCHEMAS_ENABLE: 'false'
      PLUGIN_PATH: '/kafka/connect,/usr/share/java,/usr/share/confluent-hub-components'
    healthcheck:
      test: ['CMD', 'curl', '-fsS', 'http://localhost:8083/connectors']
      interval: 10s
      timeout: 5s
      retries: 30
    # command: は削除（ENTRYPOINTに任せる）

  ksqldb:
    image: confluentinc/ksqldb-server:0.29.0
    platform: linux/amd64
    container_name: ksqldb
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - '8088:8088'
    environment:
      KSQL_BOOTSTRAP_SERVERS: kafka:9092
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_KSQL_SERVICE_ID: 'ksql-demo'
      # ← 両方にヒープ指定（どのパスでも反映されるよう保険）
      KSQL_HEAP_OPTS: '-Xms512m -Xmx1g'
      KSQL_OPTS: '-Xms512m -Xmx1g'
      KSQL_KSQL_STREAMS_AUTO_OFFSET_RESET: earliest
      # KSQL_KSQL_QUERIES_FILE: /etc/ksqldb/statements.sql
    volumes:
      - ./ksql/statements.sql:/etc/ksqldb/statements.sql:ro
    healthcheck:
      test: ['CMD', 'curl', '-fsS', 'http://localhost:8088/healthcheck']
      interval: 5s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  # Debezium コネクタ登録（ワンショット）
  connect-init:
    image: curlimages/curl:8.8.0
    container_name: connect-init
    restart: 'on-failure:3' # ← 引用
    depends_on:
      connect:
        condition: service_healthy
      pg:
        condition: service_healthy
      ksqldb: # ← 追加（KSQL叩くなら必須）
        condition: service_healthy
    entrypoint: ['/bin/sh', '-c', '/bin/sh /work/register.sh'] # ← 実行権限不要化
    environment:
      CONNECT_URL: 'http://connect:8083' # ← スクリプトで使えるように
      KSQL_URL: 'http://ksqldb:8088'
    volumes:
      - ./connect-init/register.sh:/work/register.sh:ro

  # DB書き戻し（B/C/Dトピック → Postgres upsert）
  db-writer-ab:
    build: ./db-writer-ab
    platform: linux/amd64
    container_name: db-writer-ab
    depends_on:
      kafka:
        condition: service_healthy
      pg:
        condition: service_healthy
    restart: on-failure
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC: pg.public.a_panel # ← 入力トピック
      DEST_TABLE: b_panel # ← 出力テーブル
      COLOR_MAP_JSON: '{"blue":"yellow","yellow":"blue"}'
      PGHOST: pg
      PGPORT: '5432'
      PGDATABASE: demo
      PGUSER: postgres
      PGPASSWORD: postgres

  db-writer-bc:
    build: ./db-writer-ab
    platform: linux/amd64
    container_name: db-writer-bc
    depends_on:
      kafka:
        condition: service_healthy
      pg:
        condition: service_healthy
    restart: on-failure
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC: pg.public.b_panel
      DEST_TABLE: c_panel
      COLOR_MAP_JSON: '{"yellow":"red","blue":"pink"}'
      PGHOST: pg
      PGPORT: '5432'
      PGDATABASE: demo
      PGUSER: postgres
      PGPASSWORD: postgres

  db-writer-cd:
    build: ./db-writer-ab
    platform: linux/amd64
    container_name: db-writer-cd
    depends_on:
      kafka:
        condition: service_healthy
      pg:
        condition: service_healthy
    restart: on-failure
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC: pg.public.c_panel
      DEST_TABLE: d_panel
      COLOR_MAP_JSON: '{"red":"green","pink":"purple"}'
      PGHOST: pg
      PGPORT: '5432'
      PGDATABASE: demo
      PGUSER: postgres
      PGPASSWORD: postgres

  web:
    build: ./web
    platform: linux/amd64
    container_name: web
    depends_on:
      - pg
    environment:
      PGHOST: pg
      PGPORT: '5432'
      PGDATABASE: demo
      PGUSER: postgres
      PGPASSWORD: postgres
      PORT: '8080'
    ports:
      - '8080:8080'
  ui:
    build: ./ui
    platform: linux/amd64
    container_name: ui
    depends_on:
      - web
    environment:
      - VITE_API_BASE=http://localhost:8080
    ports:
      - '5173:5173'
volumes:
  kafkadata:
